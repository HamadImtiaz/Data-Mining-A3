{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42930a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Dell/Downloads/creditcard\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4ac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "X = df.drop(columns=['Class'])\n",
    "y = df['Class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Display the shape of training and testing data\n",
    "print(\"Shape of X_train_scaled:\", X_train_scaled.shape)\n",
    "print(\"Shape of X_test_scaled:\", X_test_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e01cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def geometric_masking(data, p=0.1):\n",
    "    \"\"\"\n",
    "    Apply geometric masking to the input data.\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): Input time series data of shape (num_samples, num_features).\n",
    "    p (float): Probability of applying geometric masking.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Time series data with geometric masking applied.\n",
    "    \"\"\"\n",
    "    masked_data = np.copy(data)\n",
    "    num_samples, num_features = data.shape\n",
    "    \n",
    "    # Apply geometric masking with probability p\n",
    "    for i in range(num_samples):\n",
    "        if np.random.rand() < p:\n",
    "            mask_length = np.random.geometric(0.5)  # Geometrically distributed mask length\n",
    "            start_idx = np.random.randint(0, num_features - mask_length)\n",
    "            end_idx = start_idx + mask_length\n",
    "            masked_data[i, start_idx:end_idx] = 0\n",
    "    \n",
    "    return masked_data\n",
    "\n",
    "def random_rotation(data, max_angle=15):\n",
    "    \"\"\"\n",
    "    Apply random rotation to the input data.\n",
    "    \n",
    "    Parameters:\n",
    "    data (numpy.ndarray): Input time series data of shape (num_samples, num_features).\n",
    "    max_angle (float): Maximum rotation angle in degrees.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Time series data with random rotation applied.\n",
    "    \"\"\"\n",
    "    rotated_data = np.copy(data)\n",
    "    num_samples, num_features = data.shape\n",
    "    \n",
    "    # Apply random rotation\n",
    "    for i in range(num_samples):\n",
    "        angle = np.random.uniform(-max_angle, max_angle)\n",
    "        rotation_matrix = np.array([[np.cos(np.radians(angle)), -np.sin(np.radians(angle))],\n",
    "                                    [np.sin(np.radians(angle)), np.cos(np.radians(angle))]])\n",
    "        rotated_data[i] = np.dot(data[i].reshape(-1, 2), rotation_matrix).flatten()\n",
    "    \n",
    "    return rotated_data\n",
    "\n",
    "# Example usage:\n",
    "# Apply geometric masking\n",
    "X_train_augmented = geometric_masking(X_train_scaled, p=0.1)\n",
    "\n",
    "# Apply random rotation\n",
    "X_train_augmented = random_rotation(X_train_augmented, max_angle=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31647e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout=0, max_length=2048):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    return Model(inputs, x)\n",
    "\n",
    "# Define hyperparameters\n",
    "input_shape = X_train_augmented.shape[1:]  # Shape of input data\n",
    "head_size = 256\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "\n",
    "# Build the Transformer-based autoencoder\n",
    "autoencoder = build_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder on the augmented data\n",
    "autoencoder.fit(X_train_augmented, X_train_scaled, epochs=10, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Obtain reconstructed sequences\n",
    "reconstructed_sequences = autoencoder.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb15f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import ContrastiveLoss\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_contrastive_model(input_shape, latent_dim):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dense(latent_dim, activation='relu')(x)\n",
    "    outputs = Dense(latent_dim)(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    # Euclidean distance between the embeddings\n",
    "    distance = tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)\n",
    "    # Contrastive loss\n",
    "    return tf.reduce_mean(y_true * distance + (1 - y_true) * tf.maximum(0, margin - distance))\n",
    "\n",
    "# Define hyperparameters\n",
    "input_shape = X_train_augmented.shape[1:]  # Shape of input data\n",
    "latent_dim = 64  # Dimensionality of the latent space\n",
    "margin = 1.0  # Margin for the contrastive loss\n",
    "\n",
    "# Build the contrastive model\n",
    "contrastive_model = build_contrastive_model(input_shape, latent_dim)\n",
    "\n",
    "# Compile the model with contrastive loss\n",
    "contrastive_model.compile(optimizer=Adam(), loss=contrastive_loss)\n",
    "\n",
    "# Train the contrastive model\n",
    "contrastive_model.fit(X_train_augmented, X_train_scaled, epochs=10, batch_size=64, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a3de6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_generator(latent_dim, output_shape):\n",
    "    inputs = Input(shape=(latent_dim,))\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    outputs = Dense(np.prod(output_shape), activation='tanh')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def build_discriminator(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "def build_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    gan_output = discriminator(generator(gan_input))\n",
    "    gan = Model(gan_input, gan_output)\n",
    "    gan.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "    return gan\n",
    "\n",
    "# Define hyperparameters\n",
    "latent_dim = 100  # Dimensionality of the latent space\n",
    "output_shape = X_train_augmented.shape[1:]  # Shape of generated samples\n",
    "\n",
    "# Build and compile the generator\n",
    "generator = build_generator(latent_dim, output_shape)\n",
    "generator.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(output_shape)\n",
    "discriminator.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
    "\n",
    "# Build and compile the GAN\n",
    "gan = build_gan(generator, discriminator)\n",
    "\n",
    "# Train the GAN\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "for epoch in range(epochs):\n",
    "    # Generate random noise as input for the generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    \n",
    "    # Generate synthetic samples using the generator\n",
    "    generated_samples = generator.predict(noise)\n",
    "    \n",
    "    # Combine real and synthetic samples\n",
    "    real_samples = X_train_scaled[np.random.randint(0, X_train_scaled.shape[0], batch_size)]\n",
    "    X = np.concatenate([real_samples, generated_samples])\n",
    "    \n",
    "    # Labels for the discriminator\n",
    "    y_discriminator = np.zeros(2 * batch_size)\n",
    "    y_discriminator[:batch_size] = 1  # Labeling real samples as 1\n",
    "    \n",
    "    # Train the discriminator\n",
    "    discriminator_loss = discriminator.train_on_batch(X, y_discriminator)\n",
    "    \n",
    "    # Generate new noise for the GAN\n",
    "    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "    \n",
    "    # Labels for the generator (tricking the discriminator)\n",
    "    y_gan = np.ones(batch_size)\n",
    "    \n",
    "    # Train the GAN (only the generator)\n",
    "    gan_loss = gan.train_on_batch(noise, y_gan)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Discriminator Loss: {discriminator_loss}, GAN Loss: {gan_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1a31de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass test data through the trained autoencoder to obtain reconstruction errors\n",
    "reconstructed_sequences = autoencoder.predict(X_test_scaled)\n",
    "reconstruction_errors = np.mean(np.square(X_test_scaled - reconstructed_sequences), axis=1)\n",
    "\n",
    "# Pass test data through the trained contrastive model to obtain contrastive scores\n",
    "contrastive_scores = contrastive_model.predict(X_test_scaled)\n",
    "\n",
    "# Pass test data through the trained generator of the GAN to obtain synthetic samples\n",
    "noise = np.random.normal(0, 1, (len(X_test_scaled), latent_dim))\n",
    "generated_samples = generator.predict(noise)\n",
    "\n",
    "# Compute distances between original and generated samples\n",
    "gan_distances = np.mean(np.square(X_test_scaled - generated_samples), axis=1)\n",
    "\n",
    "# Combine anomaly scores from different models (e.g., reconstruction errors, contrastive scores, GAN distances)\n",
    "anomaly_scores = (reconstruction_errors + contrastive_scores.flatten() + gan_distances) / 3\n",
    "\n",
    "# Set threshold for anomaly detection\n",
    "threshold = np.percentile(anomaly_scores, 95)  # Adjust percentile as needed\n",
    "\n",
    "# Detect anomalies based on the threshold\n",
    "anomalies = (anomaly_scores > threshold).astype(int)\n",
    "\n",
    "# Print number of anomalies detected\n",
    "print(\"Number of anomalies detected:\", np.sum(anomalies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Assume we have ground truth labels for the test data (0 for normal, 1 for anomaly)\n",
    "# Replace `ground_truth_labels` with the actual ground truth labels\n",
    "ground_truth_labels = y_test\n",
    "\n",
    "# Compute evaluation metrics\n",
    "precision = precision_score(ground_truth_labels, anomalies)\n",
    "recall = recall_score(ground_truth_labels, anomalies)\n",
    "f1 = f1_score(ground_truth_labels, anomalies)\n",
    "roc_auc = roc_auc_score(ground_truth_labels, anomaly_scores)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ea013d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
